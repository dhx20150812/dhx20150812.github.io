---
layout:     post
title:      论文笔记
subtitle:   Fixed That for You - Generating Contrastive Claims with Semantic Edits
date:       2021-01-05
author:     dhx20150812
header-img: img/post-bg-ios9-web.jpg
catalog: true
mathjax: true
tags:
    - NLP
    - Argument Generation
    - Contrastive Claim Generation
---

# Fixed That for You: Generating Contrastive Claims with Semantic Edits

>   来自NAACL 2019, <https://www.aclweb.org/anthology/N19-1174.pdf>
>
>   代码已开源，<https://github.com/chridey/fixedthat>



## Motivation

辩论的核心是主张，即有争议的陈述。 生成反驳论点，则需要生成与原始论点的主要主张相反的回应。



## Methods

作者认为，生成相反主张的目标可以分解为两个任务：

（1）识别原主张中应该被删除或替换的词

（2）生成合适的替换词或必要的上下文

因此，作者尝试了使用双向的LSTM-CNN-CRF模型，并结合了编辑距离来获得每个字的对齐、删除或替换的标签。然而，结果表明，它的效果略优于随机的预测，由于存在误差的传播，这个模型不可能产生流畅和准确的输出。因此，受益于机器翻译技术，作者使用了一个端到端的方法。

### Model

作者使用了一个带有注意力机制的Seq2Seq模型。原主张句子中的词通过一个双向的GRU得到编码器的隐层表达 $h_i$。解码器依旧是一个GRU，在 $t$ 时间步，它从上一时刻的隐层状态 $s_{t-1}$ 和输入中生成此时的隐层状态 $s_{t}$。编码器的隐层状态组合为上下文向量：

$$
h_{t}^{*}=\sum_{i} \alpha_{t}^{i} h_{i}
$$

其中，系数 $\alpha_{t}^{i}$ 由下式计算得到：

$$
\alpha_{t}^{i}=\frac{\exp \left(h_{i}^{T} s_{t}\right)}{\sum_{s^{\prime}} \exp \left(h_{s^{\prime}}^{T} s_{t}\right)}
$$

最终得到 $t$ 时刻，词表上各词的分布为：

$$
P(w)=\operatorname{softmax}\left(V \tanh \left(W\left[s_{t} ; h_{t}^{*}\right]+b_{w}\right)+b_{v}\right)
$$

### Decoder Representation

**Counter**: 前人的研究表明，通过使用一个长度变量的向量，他们可以控制输出的长度。因此在本文中，作者使用了一个计数器变量，它的初始值是模型应该新生成的内容词的数量。在解码过程中，如果生成了一个词不在输入或停用词中，这个计数器将递减。模型使用了一个embedding向量 $e(c_t)$ 来表示每个计数，它由计数矩阵参数化得到。因此，在这个场景下，解码器的输入是 $x_t=e(w_t, c_t)$。在每个时间步，计数器的计算如下：
$$
\begin{aligned}
c_{0} &=|O \backslash(S \cup I)| \text { or desired count } \\
c_{t+1} &=\left\{\begin{array}{ll}
c_{t}-1, & w_{t} \notin S \cup I \text { and } c_{t}>0 \\
c_{t}, &\text { otherwise }
\end{array}\right.
\end{aligned}
$$

其中，$O$ 是训练阶段的标准输出。

然而，在测试阶段，无法预先得知生成的新的内容词的数量。但是，大多数FTFY的计数都在1到5之间（含1和5），因此作者尝试了在解码期间搜索此范围。实验结果表明，效果并不理想。

**Decoder Input**：作者评价了目标输出序列的两种表示方式：句子序列或编辑序列。句子序列的方法是标准的seq2seq设置。编辑序列是在原输出序列中插入`DELETE-N`，它表明之前的多少个词需要被删除。例如，对于原输出"Hillary Clinton for president 2020", 修改后的输出是”Hillary Clinton DELETE-2 Bernie Sanders for president 2020 DELETE-1“。

**Subreddit Information**： 由于模型需要消除多义词的歧义，额外的上下文也许有用。因此，作者在每个时间步为编码器传入三个类别的embedding向量，他们被拼接到原始的输入词中，$x_t=e(w_t, g_t^1,g_t^2,g_t^3)$。

### Objective Function

作者使用了负对数似然目标函数 $\mathcal{L}_{N L L}=-\log \sum_{t \in 1: T} P\left(w_{t}^{*}\right)$，其中 $w_t^*$ 是每个时间步的真实词。同时，作者添加了一个额外的损失项，它利用编码器的隐层状态做二分类预测，判断一个词是否会被复制或插入删除。作者使用了2层的MLP和二分类的交叉熵损失 $\mathcal{L}_{BCE}$。最终的损失函数为：

$$
\mathcal{L} = \mathcal{L}_{N L L} + \lambda \mathcal{L}_{B C E}
$$

### Decoding

在生成过程中，作者使用了久经检验的Beam Search方法。对于本文提出的模型需要计数器，作者将beam扩展 $m$ 倍，也就是对于beam大小为 $k$，共计算 $k*m$ 个状态。

**Filtering**: 作者使用了一个约束解码的模式。当 $c_t>1$ 时，EOS的分数被设置为 $-\infin$；当 $c_t=0$ 时，词 $w \in V \backslash(S \cup I)$ 的分数被设置为 $-\infin$。 也就是说，当计数器为0时，只允许模型复制输入词或使用停用词；当计数器为正时，可以防止模型过早地结束生成的过程。
