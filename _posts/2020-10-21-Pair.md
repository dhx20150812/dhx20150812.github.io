---
layout:     post
title:      论文笔记
subtitle:   PAIR Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation
date:       2020-08-30
author:     dhx20150812
header-img: img/post-bg-ios9-web.jpg
catalog: true
mathjax: true
tags:
    - NLP
    - 可控文本生成
    - Content Planning
---

# PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation



>   来自EMNLP 2020：https://arxiv.org/abs/2010.02301
>
>   代码已开源：https://github.com/XinyuHua/pair-emnlp2020

## Introduction

大型的预训练`Transformer`语言模型是很多NLP任务的基础，但是他们还谈不上完美。在一般的任务中，像`GPT-2`这样的模型可以生成貌似合理的文本，但无法在实际中使用，用户无法指定要包含的内容以及顺序。

在这篇文章中，作者提出了一个新的模型。它可以有效地将`content planning`结合到大型模型中，从而获得更相关和一致的文本。

首先，作者由`BERT`训练了一个`planning`模型来产生初始的`content planning`，它将关键词分配给不同的句子，并预测他们在句子中出现的位置。

然后，作者提出了一种内容可控的文本生成框架，它基于预训练的`Seq2Seq Transformer`模型`BART`，将分配的关键词和位置作为输入，编码为一个模板，没有内容的位置由`[MASK]`填充，然后输出一个流畅、一致的多句子文本。这是通过对`BART`进行微调而不修改其架构来完成的。

最后，作者提出了一个迭代细化算法来提升`seq2seq`生成文本的质量。在每次迭代中，低置信度的词将被替换为`[MASK]`来构成新的模板。具体流程见下图：

![算法流程图](https://note.youdao.com/yws/api/personal/file/WEB92b5ef04a8eaaa1f6f61f4e3ea1af796?method=download&shareKey=cfceb8148eb8f0b5d56aa6dff1b1df0b)


