---
layout:     post
title:      论文笔记
subtitle:   CoCon A Self-Supervised Approach for Controlled Text Generation
date:       2020-08-30
author:     dhx20150812
header-img: img/post-bg-ios9-web.jpg
catalog: true
mathjax: true
tags:
    - NLP
    - 可控文本生成
---


# CoCon: A Self-Supervised Approach for Controlled Text Generation

## Motivation

在使用语言模型做文本生成的任务时，给定提示文字 $x_{:t-1}=\\{x_1,\cdots,x_{t-1}\\}$，后续的文本 $\\{x_{t}, \cdots, x_{l}\\}$ 是通过自回归的方式生成的：

$$
p\left(x_{t}, \ldots, x_{l} \mid x_{1}, \ldots, x_{t-1}\right)=\prod_{i=t}^{l} p\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)
$$

之前的利用语言模型做可控文本生成任务的研究工作，如PPLM和CTRL等，都认为 $p(x)$ 可以以目标属性或控制代码为条件，来控制文本的情感或主题：

$$
p\left(x_{t}, \ldots, x_{l} \mid x_{1}, \ldots, x_{t-1}\right)=\prod_{i=1}^{l} p\left(x_{i} \mid \mathbf{a},\left\{x_{1}, \ldots, x_{i-1}\right\}\right)
$$

其中，$\mathbf{a}$ 是目标属性。这些控制生成的方法都是基于一个全局的属性（情感或主题），而不是更加局部的内容（词或词组）。因此，这激发了一种以输入目标内容 $\mathbf{c}$ 为条件的方法，以便对文本生成进行更细粒度的控制：

$$
p\left(x_{t}, \ldots, x_{l} \mid x_{1}, \ldots, x_{t-1}\right)=\prod_{i=1}^{l} p\left(x_{i} \mid \mathbf{c},\left\{x_{1}, \ldots, x_{i-1}\right\}\right)
$$


## Model Architecture

![image-20200830134110545](https://note.youdao.com/yws/api/personal/file/WEB90d6b8caea48142994e36adeebc23624?method=download&shareKey=bd86a46bdbcbac9cc8827cbc7fecfc4b)

语言模型的生成过程可以分为两个部分：编码器 $enc$ 和解码器 $dec$ 。编码器用作特征提取器，它接受输入序列的embedding，并输出中间表示 $\mathbf{h}\_{: t-1}=\operatorname{enc}\left(x\_{: t-1}\right)$。然后，解码器根据这一个中间表示产生下一个词：

$$
\mathbf{o}\_{t}=\mathrm{LM}\left(x_{: t-1}\right)=\operatorname{dec}\left(\operatorname{enc}\left(x_{: t-1}\right)\right)=\operatorname{dec}\left(\mathbf{h}_{: t-1}\right)
$$

作者提出一个CoCon块，将 $\mathbf{h}$ 转换为以目标内容 $\mathbf{c}$ 为条件的表示：

$$
\mathbf{h}_{: t-1}^{\prime}=\operatorname{CoCon}\left(\mathbf{h}_{: l_{c}}^{(\mathbf{c})}, \mathbf{h}_{: t-1}\right)
$$

其中，$\mathbf{h}\_{: l_{c}}^{(\mathbf{c})}=\operatorname{enc}(\mathbf{c})$ 是目标内容的表示，$l_c$ 是目标文本的长度。作者使用一个Transformer块来作为CoCon块。与传统的语言模型中的注意力层相似， $\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{(t-1) \times d}$ 矩阵通过中间表示的线性变换得到。为了加入内容表示 $\mathbf{h}\_{: l_{r}}^{(\mathbf{c})}$，关于内容的 $\mathbf{K}^{(\mathbf{c})}, \mathbf{V}^{(\mathbf{c})} \in \mathbb{R}^{l_{c} \times d}$ 矩阵也需要计算，然后与矩阵拼接起来：

$$
\mathbf{K}^{\prime}=\left[\mathbf{K}^{(\mathbf{c})} ; \mathbf{K}\right], \quad \mathbf{V}^{\prime}=\left[\mathbf{V}^{(\mathbf{c})} ; \mathbf{V}\right], \quad \mathbf{A}=\operatorname{Softmax}\left(\mathbf{Q} \mathbf{K}^{\prime \top}\right) \mathbf{V}^{\prime}=\operatorname{Softmax}(\mathbf{W}) \mathbf{V}^{\prime}
$$

其中，$\mathbf{A}=\\{\mathbf{a}\_{1}, \ldots, \mathbf{a}\_{t-1}\\}$ 和 $\mathbf{W} \in \mathbb{R}^{(t-1) \times\left(l_{c}+t-1\right)}$ 代表了注意力权重。CoCon块的输出由如下得到：

$$
\mathbf{h}_{i}^{\prime}=\operatorname{FF}\left(\mathbf{a}_{i}\right), \quad \tilde{\mathbf{o}}_{t}=\operatorname{dec}\left(\left[\mathbf{h}_{: t-2} ; \mathbf{h}_{t-1}^{\prime}\right]\right), \quad p_{\theta, \psi}\left(\tilde{x}_{t} \mid \mathbf{c}, x_{: t-1}\right)=\operatorname{Softmax}\left(\tilde{\mathbf{o}}_{t}\right)
$$


**Multiple Content Inputs**：CoCon允许在同一个生成过程中输入多个目标内容。假设有 $N$ 个目标内容 $\left(\mathbf{c^1},\cdots,\mathbf{c^N}\right)$，输出文本可以以注意力的键值矩阵为条件得到：

$$
\mathbf{K}^{\prime}=\left[\mathbf{K}^{\left(\mathbf{c}^{1}\right)} \ldots \mathbf{K}^{\left(\mathbf{c}^{N}\right)} ; \mathbf{K}\right], \quad \mathbf{V}^{\prime}=\left[\mathbf{V}^{\left(\mathbf{c}^{1}\right)} \ldots \mathbf{V}^{\left(\mathbf{c}^{N}\right)} ; \mathbf{V}\right], \quad \mathbf{A}=\operatorname{Softmax}\left(\mathbf{Q} \mathbf{K}^{\prime \top}\right) \mathbf{V}^{\prime}
$$


**Strength of Content Conditioning**： 可以为内容注意力权重 $\mathbf{W}\_{:,: l_{c}} \in \mathbb{R}^{(t-1) \times l\_{c}}$ 添加偏置项 $\tau\_{content}$。正的偏置项可以增强内容控制，负的偏置会减轻控制作用。

