---
layout:     post
title:      论文笔记
subtitle:   BERT论文笔记
date:       2018-12-12
author:     dhx20150812
header-img: img/post-bg-ios9-web.jpg
catalog: true
mathjax: true
tags:
    - NLP
---


# Bert论文笔记

> 本文是个人在阅读[Bert原论文](https://arxiv.org/pdf/1810.04805.pdf)的笔记，作为日后回顾时参考。

## Motivation

预训练的语言模型已被证明在许多自然语言处理任务上很有效，包括自然语言推断，命名实体识别，智能问答等。目前有两种主流的策略将预训练的语言表示应用到下游任务上，一种策略是基于特征(feature-based)的，另一种是微调(fine-tuning)。前者的典型代表是[ELMo](https://arxiv.org/pdf/1802.05365.pdf)，它使用了针对特定任务的结构，将预训练的语言表示作为额外的特征；后者的代表是[OpenAI GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)，它通过简单地微调预训练参数在下游任务中训练。两种方法在预训练期间使用相同的目标函数，他们都使用单向的语言模型来学习一般语言表示。

本文作者认为，当前的技术严重限制了预训练表示的能力，尤其是微调的方法。主要的约束就在传统的语言模型是无向的，它会限制在预训练阶段可以选择的模型结构。以OpenAI GPT为例，作者使用了一个自左向右的结构，[Transformer](https://arxiv.org/pdf/1706.03762.pdf)中的self-attention层里的每一个位置只能处理之前的位置。

## The Model Details

### Model Architecture

​	在本文中，作者提出了一个全新的模型——**BERT**（**B**idirectional **E**ncoder **R**epresentation from **T**ransformer）。我们先看下模型的结构，它由多层的双向的Transformer encoder组成。我们令Transformer的堆叠层数为$L$，隐藏层的大小为$H$，Multi-head Attention的数目为$A$，前馈神经网络的大小为$4H$。论文共提出了两种尺度的模型——

- $BERT_{BASE}: L=12,H=768,A=12,Total~Parameters=110M$
- $BERT_{LARGE}: L=24,H=1024,A=16,Total~Parameters=340M$

其中，$BERT_{BASE}$使用了和OPENAI GPT相同的模型大小，为了便于比较。关于BERT，OPENAI GPT和ELMo之间的比较见下图。

![](https://note.youdao.com/yws/api/personal/file/05355B5E8D7943F2B7B61D7A56F9AE01?method=download&shareKey=b3f7d3d44b1f8c3d553c8c59f6cdcdc9)

### Input Representation

 Bert输入的编码向量是3个嵌入特征的单位和，如下图所示，这3个嵌入特征分别为：

![](https://note.youdao.com/yws/api/personal/file/CDA46550CA864C0F9A346E7E865FCC0B?method=download&shareKey=712e738bae53192e1810f872e958b687)

1. WordPiece嵌入：WordPiece是指将单词划分为一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如图4的示例中‘playing’被拆分成了‘play’和‘ing’
2. 位置嵌入Positional embeddings：由学习得到的，支持的序列长度最多为512个token
3. 分割嵌入（Segment Embedding）：用于区分两个句子，例如B是否是A的下文（对话场景，问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1

需要注意的是：

- 每个序列的第一个token始终是一个特殊的分类嵌入向量（`[CLS]`）。该token的最终隐藏状态（即，Transformer的输出）被用作分类任务的序列表示。对于非分类任务，将忽略此向量
- 句子对被打包成一个序列。以两种方式区分句子。首先，用特殊标记（[SEP]）将它们分开。其次，添加一个learned sentence A嵌入到第一个句子的每个token中，一个sentence B嵌入到第二个句子的每个token中
- 对于单个句子输入，只使用 sentence A嵌入 

### Pre-training Tasks

与前人的工作不同，论文不使用传统的从左向右或从右向左的语言模型来预训练Bert。相反，作者使用了两种全新的无监督预训练任务。

#### Pre-training Task #1 ：Masked Language Model

Masked LM随机地从输入序列中mask了15%的wordpiece embedding，而不是像cbow一样把每个词都预测一遍。最终的损失函数只计算被mask掉的那些token。具体在操作时，并不是所有的被mask的token都被替换成`[MASK]`，而是遵循着如下的流程：

- 10%的单词被替换成别的单词

- 10%的单词保持原样不动

- 80%的单词被替换成`[MASK]`

值得注意的是，transformer encoder并不知道究竟是哪些单词被随机替换了，因此它要保持对每个输入token的分布式表征。同时，由于随机替换只发生在1.5%(10% * 15%)的token上，因此最终不会影响模型的语言理解能力。

#### Pre-training Task #2：Next Sentence Prediction
许多下游的任务，如问答和自然语言推理，都是基于理解两个句子间的关系。为此，作者提出了一个next sentence prediction的二分类任务。训练的输入是句子A和B，B有一半的几率是A的下一句，输入这两个句子，模型预测B是不是A的下一句。预训练的时候可以达到97-98%的准确度。

![](https://note.youdao.com/yws/api/personal/file/61ADDD2AAFCC42A7B4B24DDC72681D3E?method=download&shareKey=53f9378c4c40f785bc463877c5026cc1)

**注意：作者特意说了语料的选取很关键，要选用document-level的而不是sentence-level的，这样可以具备抽象连续长序列特征的能力。**

### Fine-tuning Procedure

在训练完Bert后，我们便可以将其应用到NLP的各下游任务上了。对于sequence-level的分类任务， Bert直接取第一个`[CLS]`token的最终的隐层状态 $C \in \mathbb{R}^H$，加一层权重矩阵 $W \in \mathbb{R}^{K \times H}$ 后通过softmax层预测label的概率

$$
P=softmax(CW^T)
$$

BERT和W的所有参数都经过微调，以最大化正确标签的对数概率。

其他的预测任务需要进行一些调整，如图所示：

![](https://note.youdao.com/yws/api/personal/file/B900E105BFAE451ABB75A2B21BCF9D4A?method=download&shareKey=832a7c06c2c3347325540be344638e61)

可以调整的参数如下：

- Batch size: 16, 32
- Learning rate(Adam): 5e-5, 3e-5, 2e-5
- Number of epochs: 3, 4 

作者发现，大的数据集相比于小的数据集而言，对超参的选择不敏感。同时微调是很快的，因此他建议多试一些参数的设置然后选择最好的。


## Experiments

微调的任务包括——

### 基于句子对的分类任务

- MNLI：Multi-Genre Natural Language Inference是一项大规模，众包的蕴涵分类任务。给出一对句子，目的是预测第二句话是否与第一句相关是蕴涵，矛盾或中立
- QQP：基于Quora的二分类任务，判断 Quora 上的两个问句是否表示的是一样的意思
- QNLI：这是一个问答的数据集。正样本是包含了正确答案的（问题，句子）对，而负样本是来自同一段的不包含答案的（问题，句子）对
- STS-B：预测两个句子间的相似性（包含了5个级别）
- MRPC：判断两个句子是否是语义等价的
- RTE：类似于MNLI，但是只是对蕴含关系的二分类判断，而且数据集更小
- SWAG：从四个句子中选择为可能为前句下文的那个

### 基于单个句子的分类任务

- SST-2：电影评价的情感分析
- CoLA：句子语义判断，是否是可接受的（Acceptable）

下图展示了Bert在这些数据集上的效果。

![](https://note.youdao.com/yws/api/personal/file/DCD53C25EDF047CA91B79C421DE5A4F6?method=download&shareKey=198ad11c04f1407d0b6bbdfddaa82834)

对于GLUE数据集的分类任务（MNLI，QQP，QNLI，SST-B，MRPC，RTE，SST-2，CoLA），BERT的微调方法是根据`[CLS]`标志生成一组特征向量 $C$，并通过一层全连接进行微调，损失函数根据任务类型自行设计，例如多分类的softmax或者二分类的sigmoid。

SWAG的微调方法和GLEU数据集类似，只不过其输出是4个可能选项的softmax

$$
P_i=\frac{e^{V \cdot C_i}}{\sum_{j=1}^{4}e^{V \cdot C_j}}
$$

### 问答任务

- SQuAD v1.1：给定一个句子（通常是一个问题）和一段描述文本，输出这个问题的答案，类似于做阅读理解的简答题。SQuAD的输入是问题和描述文本的句子对。输出是特征向量，通过在描述文本上接一层激活函数为softmax的全连接来获得输出文本的条件概率，全连接的输出节点个数是语料中Token的个数

$$
P_i=\frac{e^{S \cdot C_i}}{\sum_{j}e^{S \cdot C_j}}
$$

### 命名实体识别任务

- CoNLL-2003 NER：判断一个句子中的单词是不是Person，Organization，Location，Miscellaneous或者other（无命名实体）。微调CoNLL-2003 NER时将整个句子作为输入，在每个时间片输出一个概率，并通过softmax得到这个Token的实体类别。

## Ablation Studies

作者通过一系列对比试验，证明了BERT框架的各个方面的具体贡献。

### Effect of Pre-training Tasks

作者认为，BERT相比于前人的工作的最大的贡献，就是其深度的双向性和Masked LM的预训练。为了证实这一结论，作者设置了一个对比试验。作者实现了一个只有MLM而没有NSP任务的模型（No NSP）和另一个自左向右的LM并且没有NSP任务的模型（LTR & No NSP）。其超参设置和数据集均相同，将这两个模型与 $BERT_{BASE}$ 做对比。结果表明，$BERT_{BASE}$ 在多个数据集上的表现均优于后两者。同时，No NSP在各数据集上的表现均优于LTR & No NSP。结果证实了MLM预训练的有效性。

### Effect of Model Size

作者紧接着探索了模型的大小对微调任务准确度的影响。毫不意外，大的模型在各个任务上均比小的模型效果好一些。众所周知，增加模型的尺寸将会持续改进机器翻译和语言建模等大型任务的表现，但这是第一次证明了，如果模型已经经过充分的预训练，那么非常大的模型也可以在非常小的任务上有大幅度的改进。

### Effect of Number of Training Steps

作者分析了训练次数和MNLI Dev训练集上的准确度的关系，如下所示

![](https://note.youdao.com/yws/api/personal/file/A2752EF7A90549A5B7664BA2ABF6EE0A?method=download&shareKey=dabcaacd48209293a29b4e5039dfe54f)

作者认为很有必要使用如此大规模的训练次数，尽管MLM比LTR的收敛速度稍慢，但带来的准确度的提升是更有价值的。
