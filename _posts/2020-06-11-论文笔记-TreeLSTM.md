---
layout:     post
title:      论文笔记
subtitle:   Improved Semantic Representations From Tree Structured Long Short Term Memory Networks
date:       2020-06-11
author:     dhx20150812
header-img: img/post-bg-ios9-web.jpg
catalog: true
mathjax: true
tags:
    - NLP
    - 情感分类
---



# Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks



>   来自ACL 2015
>
>   代码已开源，https://github.com/stanfordnlp/treelstm
>
>   第三方TensorFlow实现：https://github.com/tensorflow/fold/blob/master/tensorflow_fold/g3doc/sentiment.ipynb
>
>   第三方Pytorch实现：https://github.com/dasguptar/treelstm.pytorch

## Introduction

对句子做分布式表示的模型大体上可以分为三种：词袋模型、序列模型和树结构模型。在词袋模型中，短语和句子的表示与单词顺序无关；序列模型将句子的表示构造为序列的顺序敏感函数；树结构模型根据给定的句子句法结构，从其组成子短语组成每个短语和句子的表示形式。

顺序不敏感的模型不足以完全捕获自然语言的语义，因为它们无法解决由于单词顺序或句法结构不同而造成的语义差异。例如句子“cats climb trees”和句子“trees climb cats”。因此，我们转向顺序敏感的序列模型或树结构模型。特别地，由于树结构模型与句子结构的句法解释之间的关系，因此它们更具有吸引力。于是自然有一个问题：与句子表示的序列模型相比，树结构模型在多大程度上可以做得更好？

在这篇文章中，作者直接比较了最近在几种NLP任务上实现了SOTA结果的序列模型及其树结构模型。作者将标准LSTM结构迁移到树结构中，并展示了其在表达句子含义时相比于序列LSTM模型的优越性。标准LSTM会根据当前时间步的输入和上一个时间步的LSTM单元的隐层状态来构成其当前的隐层状态。而树结构的LSTM会根据输入向量和所有孩子节点的隐层状态构成其当前的隐层状态。标准的LSTM结构可以看作是树结构LSTM的一个特例，它的每个内部节点只有一个孩子。

最后作者在两个任务上对比了模型的效果——语义关联和情感分类。结果表明树结构的LSTM在两个任务上都优于序列的LSTM。

## LSTM

标准的LSTM通过引入能够长时间保持状态的存储单元，解决了学习长期依赖的问题。一个标准的LSTM层包含了一个输入门 $i_t$，一个遗忘门 $f_t$，一个记忆单元 $c_t$，一个输出门 $o_t$ 和一个隐层状态 $h_t$。LSTM的变换公式如下所示：

$$
\begin{aligned} i_{t} &=\sigma\left(W^{(i)} x_{t}+U^{(i)} h_{t-1}+b^{(i)}\right) \\ f_{t} &=\sigma\left(W^{(f)} x_{t}+U^{(f)} h_{t-1}+b^{(f)}\right) \\ o_{t} &=\sigma\left(W^{(o)} x_{t}+U^{(o)} h_{t-1}+b^{(o)}\right) \\ u_{t} &=\tanh \left(W^{(u)} x_{t}+U^{(u)} h_{t-1}+b^{(u)}\right) \\ c_{t} &=i_{t} \odot u_{t}+f_{t} \odot c_{t-1} \\ h_{t} &=o_{t} \odot \tanh \left(c_{t}\right) \end{aligned}
$$

其中，$x_t$ 是当前时刻的输入，$\sigma$ 是logistic函数，$\odot$ 是元素乘法。遗忘门控制了前一个记忆单元被遗忘的程度，输入门控制了每个单元更新的程度，输出门控制了内部的记忆状态的暴露。

## Tree-Structured LSTMs

标准的LSTM的缺陷是，他们只允许严格的序列信息的传播。在这篇文章中，作者针对这一问题提出了两种改型——Child-Sum Tree-LSTM和N-ary Tree-LSTM，两种变体都使用了更丰富的网络拓扑结构，其中每个LSTM单元都能够合并来自多个子单元的信息。

每个Tree-LSTM单元（下标为 $j$）包含输入输出门 $i_j$ 和 $o_j$，一个记忆单元 $c_j$ 和隐层状态 $h_j$。Tree-LSTM与标准的LSTM的区别在于，它的门控向量和记忆单元的更新取决于很多可能的子单元的状态。而且，不同于只有一个遗忘门，Tree-LSTM单元对于每个子单元都有一个遗忘门 $f_{jk}$，这使得Tree-LSTM可以选择性地从每个孩子中合并信息。

### Child-Sum Tree-LSTMs

给定一棵树，令 $C(j)$ 为节点 $j$ 的孩子集合，Child-Sum Tree-LSTM的公式如下：

$$
\begin{aligned} \tilde{h}_{j} &=\sum_{k \in C(j)} h_{k} \\ i_{j} &=\sigma\left(W^{(i)} x_{j}+U^{(i)} \tilde{h}_{j}+b^{(i)}\right) \\ f_{j k} &=\sigma\left(W^{(f)} x_{j}+U^{(f)} h_{k}+b^{(f)}\right) \\ o_{j} &=\sigma\left(W^{(o)} x_{j}+U^{(o)} \tilde{h}_{j}+b^{(o)}\right) \\ u_{j} &=\tanh \left(W^{(u)} x_{j}+U^{(u)} \tilde{h}_{j}+b^{(u)}\right) \\ c_{j} &=i_{j} \odot u_{j}+\sum_{k \in C(j)} f_{j k} \odot c_{k} \\ h_{j} &=o_{j} \odot \tanh \left(c_{j}\right) \end{aligned}
$$

由于 Child-Sum Tree-LSTM 单元能根据子节点隐藏状态 $h_k$ 的总和来确定其组成，因此它非常适合分支因子较高或子节点无序的区域。例如，它适用于依存树，它的头节点的孩子数目有较高的变化。于是作者将 Child-Sum Tree-LSTM 应用到依存树，作为Dependency Tree-LSTM。

###  $N$-ary Tree-LSTM

$N$-ary Tree-LSTM 可用于分支数最大为 $N$，且孩子全都有序的树结构中。对于任意的一个节点 $j$， $N$-ary Tree-LSTM 的变换公式为：

$$
\begin{aligned} i_{j} &=\sigma\left(W^{(i)} x_{j}+\sum_{\ell=1}^{N} U_{\ell}^{(i)} h_{j \ell}+b^{(i)}\right) \\ f_{j k} &=\sigma\left(W^{(f)} x_{j}+\sum_{\ell=1}^{N} U_{k \ell}^{(f)} h_{j \ell}+b^{(f)}\right) \\ o_{j} &=\sigma\left(W^{(o)} x_{j}+\sum_{\ell=1}^{N} U_{\ell}^{(o)} h_{j \ell}+b^{(o)}\right) \\ u_{j} &=\tanh \left(W^{(u)} x_{j}+\sum_{\ell=1}^{N} U_{\ell}^{(u)} h_{j \ell}+b^{(u)}\right) \\ c_{j} &=i_{j} \odot u_{j}+\sum_{\ell=1}^{N} f_{j \ell} \odot c_{j \ell} \\ h_{j} &=o_{j} \odot \tanh \left(c_{j}\right) \end{aligned}
$$

为每个孩子 $k$ 引入单独的参数矩阵，这使得 $N$-ary Tree-LSTM 可以比 Child-Sum Tree-LSTM 学习到更多关于孩子隐层状态的细粒度信息。例如，一个成分分析树，其中一个节点的左孩子可能是一个名词短语，右孩子是一个动词短语。在这种情况下，在树的表示中强调动词短语是有利的。于是可以训练 $U_{k \ell}^{(f)}$ 参数，使得$f_{k1}$ 的成分接近0（表示遗忘），$f_{k2}$ 的成分接近1（表示保留）。

作者将 Binary Tree-LSTM 单元应用于成分分析树，因为左右子节点是有区别的。 我们将 Binary Tree-LSTM 的这种应用称为 Constituency Tree-LSTM。 值得注意的是，在 Constituency Tree-LSTM 中，当仅当节点 $j$ 是叶节点时，它才接收输入向量 $x_j$。在本文的剩余部分，我们只关注 Dependency Tree-LSTMs 和 Constituency Tree-LSTMs。

## Models

### Tree-LSTM 分类

我们希望对树的部分节点从离散的类别集合 $\mathcal{Y}$ 预测标签 $\hat{y}$。对于节点 $j$，给定输入$\{x\}_j$，我们使用一个softmax分类器来预测标签 $\hat{y}_j$。分类器将隐层状态 $h_j$ 作为输入：

$$
\begin{aligned} \hat{p}_{\theta}\left(y |\{x\}_{j}\right) &=\operatorname{softmax}\left(W^{(s)} h_{j}+b^{(s)}\right) \\ \hat{y}_{j} &=\arg \max _{y} \hat{p}_{\theta}\left(y |\{x\}_{j}\right) \end{aligned}
$$

损失函数是真实标签的负对数似然：

$$
J(\theta)=-\frac{1}{m} \sum_{k=1}^{m} \log \hat{p}_{\theta}\left(y^{(k)} |\{x\}^{(k)}\right)+\frac{\lambda}{2}\|\theta\|_{2}^{2}
$$

$m$ 是训练集中标记节点的数目，上标 $k$ 表示第 $k$ 个标记节点，$\lambda$ 是L2正则超参。

### 句子对的语义关联

给定一个句子对，我们希望预测得到一个实值相似度评分，它的范围是 $[1, K]$。我们先使用 Tree-LSTM 模型从每个句子的解析树中得到句子表示 $h_L$ 和 $h_R$，然后利用神经网络，综合考虑句子对 $\left(h_{L}, h_{R}\right)$ 的距离和角度得到最终的相似度评分$\hat{y}$。

$$
\begin{aligned} h_{\times} &=h_{L} \odot h_{R} \\ h_{+} &=\left|h_{L}-h_{R}\right| \\ h_{s} &=\sigma\left(W^{(\times)} h_{\times}+W^{(+)} h_{+}+b^{(h)}\right) \\ \hat{p}_{\theta} &=\operatorname{softmax}\left(W^{(p)} h_{s}+b^{(p)}\right) \\ \hat{y} &=r^{T} \hat{p}_{\theta} \end{aligned}
$$

给定模型参数 $\theta$，我们希望得到的排名接近真实的排名 $
y \in[1, K]: \hat{y}=r^{T} \hat{p}_{\theta} \approx y$。于是先定义一个稀疏的目标分步 $p$ 满足 $y=r^Tp$：

$$
p_{i}=\left\{\begin{array}{ll}y-\lfloor y\rfloor, & i=\lfloor y\rfloor+1 \\ \lfloor y\rfloor-y+1, & i=\lfloor y\rfloor \\ 0 & \text { otherwise }\end{array}\right.
$$

其中$1\le i \le K$，损失函数定义为 $p$ 与 $\hat{p}_{\theta}$ 的KL散度：

$$
J(\theta)=\frac{1}{m} \sum_{k=1}^{m} \mathrm{KL}\left(p^{(k)} \| \hat{p}_{\theta}^{(k)}\right)+\frac{\lambda}{2}\|\theta\|_{2}^{2}
$$

其中，$m$ 是训练样本对的数目，上标 $k$ 是第 $k$ 个句子对。

## 实验

作者在两个任务上进行了实验：（1）情感分类和（2）语义关联。不同的模型如下：

![image-20200611103354350](https://note.youdao.com/yws/api/personal/file/WEB4d480ec22ee6940767526be350110c73?method=download&shareKey=a5d931bb3d878aeea5b5db2e60d7728c)

### 情感分类

这个任务的设置是，对电影评论数据集（SST）中抽取的句子预测情感。它有两个子任务，一个是二分类，另一个是对五个类别（非常消极、消极、中性、积极和非常积极）的细粒度的分类。

对序列LSTM而言，我们用最后一个隐藏层状态来预测情感。然后我们使用解析树来构建语法树，对于依存树，每个句子都会产生依存解析树。如果树的节点的广度与训练集中标记的跨度广配，则为该树的该节点赋予一个情感标签。

### 语义关联

对给定的句子对，语义关联的任务要求是预测出两个句子的语义相似度。作者使用了SICK数据集，共包含9927个句子对，每个句子对都标记了一个相似度评分 $y \in [1,5]$。作者在这个任务上也进行了多个实验。

## 实验结果

### 情感分类

这一任务的实验结果如下所示。在细粒度分类的子任务上，Constituency Tree-LSTM的表现优于所有现有的模型。特别地，我们发现Constituency Tree-LSTM优于Dependency Tree-LSTM，这可能由于Dependency Tree-LSTM在更少的数据集上训练导致的。

![image-20200611110241455](https://note.youdao.com/yws/api/personal/file/WEBecbcd52d4b85a6e80da8f2eeb1764850?method=download&shareKey=38244c80023cda25a83602c43d29c539)

同时，我们发现在训练时更新词向量的表示也会提高效果。这是因为Glove词向量在训练时没有捕获情感信息。

### 语义关联

该任务的实验结果罗列如下：

![image-20200611214711259](https://note.youdao.com/yws/api/personal/file/WEB0abfd7372cd7c48c918e85919821e833?method=download&shareKey=8932191f746dc05b41962c891db98baf)

可见，在不需要特征工程的前提下，Dependency Tree-LSTM取得了最好的效果。
