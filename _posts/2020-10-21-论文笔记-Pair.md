---
layout:     post
title:      论文笔记
subtitle:   PAIR Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation
date:       2020-10-21
author:     dhx20150812
header-img: img/post-bg-ios9-web.jpg
catalog: true
mathjax: true
tags:
    - NLP
    - 可控文本生成
    - Content Planning
---

# PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation



>   来自EMNLP 2020：<https://arxiv.org/abs/2010.02301>
>
>   代码已开源：<https://github.com/XinyuHua/pair-emnlp2020>

## Introduction

大型的预训练`Transformer`语言模型是很多NLP任务的基础，但是他们还谈不上完美。在一般的任务中，像`GPT-2`这样的模型可以生成貌似合理的文本，但无法在实际中使用，用户无法指定要包含的内容以及顺序。

在这篇文章中，作者提出了一个新的模型。它可以有效地将`content planning`结合到大型模型中，从而获得更相关和一致的文本。

首先，作者由`BERT`训练了一个`planning`模型来产生初始的`content planning`，它将关键词分配给不同的句子，并预测他们在句子中出现的位置。

然后，作者提出了一种内容可控的文本生成框架，它基于预训练的`Seq2Seq Transformer`模型`BART`，将分配的关键词和位置作为输入，编码为一个模板，没有内容的位置由`[MASK]`填充，然后输出一个流畅、一致的多句子文本。这是通过对`BART`进行微调而不修改其架构来完成的。

最后，作者提出了一个迭代细化算法来提升`seq2seq`生成文本的质量。在每次迭代中，低置信度的词将被替换为`[MASK]`来构成新的模板。具体流程见下图：

![算法流程图](https://note.youdao.com/yws/api/personal/file/WEB92b5ef04a8eaaa1f6f61f4e3ea1af796?method=download&shareKey=cfceb8148eb8f0b5d56aa6dff1b1df0b)


## Content-controlled Text Generation with PAIR

### 任务描述

输入包括（1）一个句子级别的`prompt` $x$，例如是一个新闻标题或者辩论中的论题和（2）与`prompt` $x$相关的一组关键字 $m$。作者希望生成 $y$，它能够包含多个句子，例如新闻报道或辩论，可以一致地反映这组关键词。

### Content Planning with BERT

作者选择`BERT`来将关键词分配给不同的句子并预测他们的位置。如下图所示，`prompt` $x$ 和无序关键词组 $m$ 后由双向`self attention`编码。关键词组的分配 $\boldsymbol{m}^{\prime}=\\{w_{j}\\}$ 将以自回归的方式进行，他们在句子 $\boldsymbol{s}=\\{s_{j}\\}$ 的位置预测被视作一个序列标注问题。

![image-20201021232141535](https://note.youdao.com/yws/api/personal/file/WEB1ffd3278cf7b234377bbe442fa969fe0?method=download&shareKey=296056d4ab751496c7b3577d730600d7)

作者利用BERT中的`segment embedding`来区别输入与输出的序列。具体来说，作者复用了预训练模型的输出层来做关键词的分配；然后作者设计了一个分离的`keyphrase assignment layer`来预测每个词在句子中的位置（也就是距离句子开头的相对位置）：

$$
p\left(s_{j} \mid \boldsymbol{w}_{\leq j}\right)=\operatorname{softmax}\left(\boldsymbol{H}^{L} \boldsymbol{W}_{s}\right)
$$

其中，$\boldsymbol{H}^{L}$ 是最后一层`Transformer`的隐层状态，$\boldsymbol{W}_{s}$是需要在微调时学习的参数。合法的位置的范围是0到127。

由于预测结果是自回归的方式得到的，因此注意力应该只考虑已经生成的词，而不是未来的词。而BERT使用了双向的注意力机制，为了解决这个问题，作者使用了`causal attention mask`来避免连接到未来的词上。

`[BOK]` 标志着关键词分配生成的开始。作者使用了一种贪心的解码算法，它限制了输出词表，保证 $m$ 中的每个词最多只会生成一次。为了完成进行句子级别的`content planning`，算法将生成一个特殊的`[SEN]`来表示句子边界，其预测的位置表示长度。 生成`[EOS]`时，生成过程停止。

### Decoding

给定 `content planning` 模型，作者使用其为不同的句子生成 `keyphase` 、对应的位置 $m^{'}$和每个句子的长度。首先，作者先将`plan`转换为模板 $t^{(0)}$：对于每个句子，将指定的关键词短语放置在其预测位置，并用`[MASK]`符号填充空白位置。

`prompt` $\boldsymbol{x}$、分配的关键词 $m^{'}$ 和模板 $t^{(0)}$ 拼接后一起输入到编码器中，解码器依据概率分布 $p\left(\boldsymbol{y}^{(1)} \mid \boldsymbol{x}, \boldsymbol{m}^{\prime}, \boldsymbol{t}^{(0)}\right)$ 生成输出 $\boldsymbol{y}^{(1)}$。$\boldsymbol{y}^{(1)}$ 是一份草稿，将在后续的步骤中进一步完善。作者使用了 `BART` 完成解码。

### Iterative Refinement

单轮的输出可能存在错误或者不连贯的问题，因此，作者提出了一种迭代优化的策略来提升生成质量。在每一轮里，$n$ 个具有最低置信度的词将被替换为`[MASK]`，于是 `MASK` 后的句子将成为新的模板。作者逐渐减少`[MASK]`的数量，在5轮迭代中，线性地从80%的$\left\|\boldsymbol{y}^{(r)}\right\|$减少到0。

作者的训练方案与 `MASK` 语言模型的训练相似。给定训练语料 $\mathcal{D}=\\{\(\boldsymbol{x}\_{i}, \boldsymbol{m}\_{i}^{\prime}, \boldsymbol{y}\_{i}\)\\}$，作者考虑了两种方案来为目标输出增加噪声：（1）随机地mask部分词（2）随机地mask不在关键词扩展集里的部分词。后者与作者的解码目标更契合。作者将$\boldsymbol{x}\_{i}$、$\boldsymbol{m}^{\prime}$ 和随机替换后输出 $\tilde{\boldsymbol{y}}\_{i}$ 作为输入，然后微调 `BART` 根据交叉熵损失来重构原目标$\boldsymbol{y}\_{i}$。
