---
layout:     post
title:      论文笔记
subtitle:   PAIR Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation
date:       2020-10-21
author:     dhx20150812
header-img: img/post-bg-ios9-web.jpg
catalog: true
mathjax: true
tags:
    - NLP
    - 可控文本生成
    - Content Planning
---

# PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation



>   来自EMNLP 2020：<https://arxiv.org/abs/2010.02301>
>
>   代码已开源：<https://github.com/XinyuHua/pair-emnlp2020>

## Introduction

大型的预训练`Transformer`语言模型是很多NLP任务的基础，但是他们还谈不上完美。在一般的任务中，像`GPT-2`这样的模型可以生成貌似合理的文本，但无法在实际中使用，用户无法指定要包含的内容以及顺序。

在这篇文章中，作者提出了一个新的模型。它可以有效地将`content planning`结合到大型模型中，从而获得更相关和一致的文本。

首先，作者由`BERT`训练了一个`planning`模型来产生初始的`content planning`，它将关键词分配给不同的句子，并预测他们在句子中出现的位置。

然后，作者提出了一种内容可控的文本生成框架，它基于预训练的`Seq2Seq Transformer`模型`BART`，将分配的关键词和位置作为输入，编码为一个模板，没有内容的位置由`[MASK]`填充，然后输出一个流畅、一致的多句子文本。这是通过对`BART`进行微调而不修改其架构来完成的。

最后，作者提出了一个迭代细化算法来提升`seq2seq`生成文本的质量。在每次迭代中，低置信度的词将被替换为`[MASK]`来构成新的模板。具体流程见下图：

![算法流程图](https://note.youdao.com/yws/api/personal/file/WEB92b5ef04a8eaaa1f6f61f4e3ea1af796?method=download&shareKey=cfceb8148eb8f0b5d56aa6dff1b1df0b)


## Content-controlled Text Generation with PAIR

### 任务描述

输入包括（1）一个句子级别的`prompt` $x$，例如是一个新闻标题或者辩论中的论题和（2）与`prompt` $x$相关的一组关键字 $m$。作者希望生成 $y$，它能够包含多个句子，例如新闻报道或辩论，可以一致地反映这组关键词。

### Content Planning with BERT

作者选择`BERT`来将关键词分配给不同的句子并预测他们的位置。如下图所示，`prompt` $x$ 和无序关键词组 $m$ 后由双向`self attention`编码。关键词组的分配 $\boldsymbol{m}^{\prime}=\left\{w_{j}\right\}$ 将以自回归的方式进行，他们在句子 $\boldsymbol{s}=\left\{s_{j}\right\}$ 的位置预测被视作一个序列标注问题。

![image-20201021232141535](https://note.youdao.com/yws/api/personal/file/WEB1ffd3278cf7b234377bbe442fa969fe0?method=download&shareKey=296056d4ab751496c7b3577d730600d7)

作者利用BERT中的`segment embedding`来区别输入与输出的序列。具体来说，作者复用了预训练模型的输出层来做关键词的分配；然后作者设计了一个分离的`keyphrase assignment layer`来预测每个词在句子中的位置（也就是距离句子开头的相对位置）：

$$
p\left(s_{j} \mid \boldsymbol{w}_{\leq j}\right)=\operatorname{softmax}\left(\boldsymbol{H}^{L} \boldsymbol{W}_{s}\right)
$$

其中，$\boldsymbol{H}^{L}$ 是最后一层`Transformer`的隐层状态，$\boldsymbol{W}_{s}$是需要在微调时学习的参数。合法的位置的范围是0到127。

由于预测结果是自回归的方式得到的，因此注意力应该只考虑已经生成的词，而不是未来的词。而BERT使用了双向的注意力机制，为了解决这个问题，作者使用了`causal attention mask`来避免连接到未来的词上。

`[BOK]` 标志着关键词分配生成的开始。作者使用了一种贪心的解码算法，它限制了输出词表，保证 $m$ 中的每个词最多只会生成一次。为了完成进行句子级别的`content planning`，算法将生成一个特殊的`[SEN]`来表示句子边界，其预测的位置表示长度。 生成`[EOS]`时，生成过程停止。


